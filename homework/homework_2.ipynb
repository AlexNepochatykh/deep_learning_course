{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Реализовать обучение линейной регрессии для задачи boston house prices \n",
    "(https://www.kaggle.com/vikrishnan/boston-house-prices) с использованием torch’а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS','NOX','RM', 'AGE','DIS', 'RAD','TAX', 'PTRATIO','B', 'LSTAT', 'MEDV'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = X.shape[1]\n",
    "num_examples = X.shape[0]\n",
    "w = torch.randn(X.shape[1]) # todo - xavie initializing\n",
    "b = torch.rand(1)# todo - xavie initializing\n",
    "features_train, features_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "labels_train, labels_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features) # число 100\n",
    "    indices = list(range(num_examples)) # список чисел от 0 до 99\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices) # перемешиваем этот список\n",
    "    for i in range(0, num_examples, batch_size): \n",
    "        j = indices[i: min(i + batch_size, num_examples)] # Выбираем индексы для батчка\n",
    "        yield features[j, :], labels[j] # возвращаем батч"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тест маленького батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5512,  4.3177, -1.3602, -0.2513, -1.2520,  2.6566, -1.4106,  0.7376,\n",
      "         -0.6510, -1.1328, -1.8325,  0.4114, -1.4210],\n",
      "        [ 1.6868, -0.4778,  1.0755, -0.2513,  1.5400,  0.4286,  0.7771, -0.8795,\n",
      "          1.7709,  1.6481,  0.7361, -3.4216,  0.6265],\n",
      "        [-0.5223,  0.6327, -0.8579, -0.2513, -1.1236, -0.8009,  0.1785,  2.2102,\n",
      "         -0.2877, -0.4659,  0.2224,  0.2208, -0.1226],\n",
      "        [-0.3438, -0.4778, -0.4977, -0.2513, -0.2013, -0.1224,  1.0159,  0.2481,\n",
      "         -0.6510, -0.6106,  1.1098,  0.2655, -0.0497],\n",
      "        [-0.4296, -0.4778, -0.4977, -0.2513, -0.2013, -0.9949, -1.2431,  0.0517,\n",
      "         -0.6510, -0.6106,  1.1098, -0.6608, -0.2319],\n",
      "        [-0.5427,  1.1880, -1.4392, -0.2513, -0.7702,  1.7869,  0.0146, -0.3103,\n",
      "         -0.2877, -1.1454, -0.1045,  0.4785, -0.9365],\n",
      "        [-0.4026, -0.4778,  1.6742, -0.2513,  0.5401, -0.5686,  0.9589, -0.7013,\n",
      "         -0.6510,  0.2073,  1.2032, -0.9377,  0.5266],\n",
      "        [ 0.4471, -0.4778,  1.0755, -0.2513,  0.1953,  0.3776,  0.1180, -0.7769,\n",
      "          1.7709,  1.6481,  0.7361, -2.6777, -0.1860],\n",
      "        [ 1.5219, -0.4778,  1.0755, -0.2513,  1.1348, -0.3845,  0.8270, -0.9940,\n",
      "          1.7709,  1.6481,  0.7361,  0.4785,  0.3970],\n",
      "        [-0.5458,  1.1880, -1.4392, -0.2513, -0.7702,  0.6481, -0.4771, -0.1697,\n",
      "         -0.2877, -1.1454, -0.1045,  0.4411, -0.6044]]) tensor([50.0000,  9.6000, 17.6000, 14.5000, 20.2000, 33.4000, 15.6000, 16.1000,\n",
      "        12.7000, 28.4000])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for X, y in data_iter(batch_size, features_train, labels_train):\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(13, 1))\n",
    "loss = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mi\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\mi\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\mi\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, taked: 0.02345,train -> loss: 59.1282321438931, test -> loss: 59.09167,\n",
      "epoch 1, taked: 0.01699,train -> loss: 54.05265959182588, test -> loss: 57.06777,\n",
      "epoch 2, taked: 0.01197,train -> loss: 55.00312971360613, test -> loss: 54.85545,\n",
      "epoch 3, taked: 0.01296,train -> loss: 70.18394455107132, test -> loss: 52.9015,\n",
      "epoch 4, taked: 0.01135,train -> loss: 50.275262964834084, test -> loss: 51.48252,\n",
      "epoch 5, taked: 0.01197,train -> loss: 47.79214024307704, test -> loss: 49.12699,\n",
      "epoch 6, taked: 0.012,train -> loss: 47.39562791880995, test -> loss: 47.49035,\n",
      "epoch 7, taked: 0.012,train -> loss: 45.21278819471303, test -> loss: 46.06177,\n",
      "epoch 8, taked: 0.01093,train -> loss: 44.880788973062344, test -> loss: 45.46998,\n",
      "epoch 9, taked: 0.01196,train -> loss: 43.563815579556, test -> loss: 42.93736,\n",
      "epoch 10, taked: 0.01101,train -> loss: 43.640701596099554, test -> loss: 41.27314,\n",
      "epoch 11, taked: 0.01194,train -> loss: 39.4257113012937, test -> loss: 39.98873,\n",
      "epoch 12, taked: 0.01092,train -> loss: 38.44157394560257, test -> loss: 39.23997,\n",
      "epoch 13, taked: 0.01097,train -> loss: 36.89981411471225, test -> loss: 37.55053,\n",
      "epoch 14, taked: 0.012,train -> loss: 39.221268757735146, test -> loss: 36.8848,\n",
      "epoch 15, taked: 0.01101,train -> loss: 35.822189482131805, test -> loss: 35.06858,\n",
      "epoch 16, taked: 0.01094,train -> loss: 32.89602525163405, test -> loss: 34.23504,\n",
      "epoch 17, taked: 0.01097,train -> loss: 45.948946055799425, test -> loss: 32.43103,\n",
      "epoch 18, taked: 0.01196,train -> loss: 31.56430960173654, test -> loss: 31.67023,\n",
      "epoch 19, taked: 0.011,train -> loss: 30.571354932124073, test -> loss: 30.75498,\n",
      "epoch 20, taked: 0.01097,train -> loss: 30.24976084491994, test -> loss: 29.9702,\n",
      "epoch 21, taked: 0.00998,train -> loss: 28.631889569877398, test -> loss: 29.11964,\n",
      "epoch 22, taked: 0.01001,train -> loss: 29.01158549998066, test -> loss: 28.17198,\n",
      "epoch 23, taked: 0.01072,train -> loss: 28.82741387999884, test -> loss: 27.75927,\n",
      "epoch 24, taked: 0.00981,train -> loss: 38.12193283232132, test -> loss: 26.40087,\n",
      "epoch 25, taked: 0.01094,train -> loss: 26.020141148331142, test -> loss: 25.82309,\n",
      "epoch 26, taked: 0.01097,train -> loss: 24.94421371611038, test -> loss: 25.05235,\n",
      "epoch 27, taked: 0.01093,train -> loss: 24.406877574354116, test -> loss: 24.52497,\n",
      "epoch 28, taked: 0.01197,train -> loss: 24.14184381466101, test -> loss: 23.65252,\n",
      "epoch 29, taked: 0.01095,train -> loss: 22.27123552265734, test -> loss: 23.41855,\n",
      "epoch 30, taked: 0.01097,train -> loss: 21.924100233776734, test -> loss: 22.65589,\n",
      "epoch 31, taked: 0.01047,train -> loss: 21.748833986792235, test -> loss: 22.15751,\n",
      "epoch 32, taked: 0.01101,train -> loss: 21.509862692049236, test -> loss: 21.99016,\n",
      "epoch 33, taked: 0.01101,train -> loss: 20.073561300145517, test -> loss: 21.22403,\n",
      "epoch 34, taked: 0.00998,train -> loss: 19.800960016722726, test -> loss: 21.14434,\n",
      "epoch 35, taked: 0.00974,train -> loss: 19.535348382326635, test -> loss: 20.44291,\n",
      "epoch 36, taked: 0.01093,train -> loss: 30.668763906648845, test -> loss: 19.79569,\n",
      "epoch 37, taked: 0.00981,train -> loss: 28.911393873762375, test -> loss: 18.78615,\n",
      "epoch 38, taked: 0.01093,train -> loss: 18.609487930146773, test -> loss: 18.37059,\n",
      "epoch 39, taked: 0.01101,train -> loss: 17.304024748282856, test -> loss: 18.08498,\n",
      "epoch 40, taked: 0.01097,train -> loss: 18.384149041506323, test -> loss: 17.71904,\n",
      "epoch 41, taked: 0.01038,train -> loss: 18.93831970668075, test -> loss: 17.41005,\n",
      "epoch 42, taked: 0.01197,train -> loss: 17.131237577683855, test -> loss: 16.99595,\n",
      "epoch 43, taked: 0.01099,train -> loss: 16.728477515796623, test -> loss: 16.75001,\n",
      "epoch 44, taked: 0.01001,train -> loss: 16.80753443500783, test -> loss: 16.18157,\n",
      "epoch 45, taked: 0.01297,train -> loss: 15.630385332768507, test -> loss: 15.87297,\n",
      "epoch 46, taked: 0.01134,train -> loss: 15.82956842856832, test -> loss: 15.81212,\n",
      "epoch 47, taked: 0.01097,train -> loss: 16.59697470334497, test -> loss: 15.59477,\n",
      "epoch 48, taked: 0.01097,train -> loss: 15.29193383396262, test -> loss: 15.45146,\n",
      "epoch 49, taked: 0.01005,train -> loss: 14.54412059854753, test -> loss: 14.96139,\n",
      "epoch 50, taked: 0.011,train -> loss: 14.603869428729066, test -> loss: 14.65882,\n",
      "epoch 51, taked: 0.01097,train -> loss: 14.221086066548187, test -> loss: 14.81045,\n",
      "epoch 52, taked: 0.01097,train -> loss: 14.156426316440696, test -> loss: 14.1592,\n",
      "epoch 53, taked: 0.01078,train -> loss: 14.399313350715259, test -> loss: 13.94532,\n",
      "epoch 54, taked: 0.0108,train -> loss: 13.735383789138039, test -> loss: 13.99315,\n",
      "epoch 55, taked: 0.01101,train -> loss: 13.77287485103796, test -> loss: 13.56033,\n",
      "epoch 56, taked: 0.00996,train -> loss: 13.387749010973637, test -> loss: 13.50991,\n",
      "epoch 57, taked: 0.01097,train -> loss: 13.274380976610844, test -> loss: 13.26328,\n",
      "epoch 58, taked: 0.0106,train -> loss: 15.141154166495445, test -> loss: 13.11677,\n",
      "epoch 59, taked: 0.01097,train -> loss: 13.600669539800965, test -> loss: 12.77484,\n",
      "epoch 60, taked: 0.00997,train -> loss: 13.85218049984167, test -> loss: 12.79634,\n",
      "epoch 61, taked: 0.012,train -> loss: 12.681963193534624, test -> loss: 12.77968,\n",
      "epoch 62, taked: 0.01095,train -> loss: 21.151039010227315, test -> loss: 12.20305,\n",
      "epoch 63, taked: 0.01097,train -> loss: 13.274434778950003, test -> loss: 11.95015,\n",
      "epoch 64, taked: 0.01101,train -> loss: 12.021484484766969, test -> loss: 12.02502,\n",
      "epoch 65, taked: 0.01101,train -> loss: 11.88723399969611, test -> loss: 11.94688,\n",
      "epoch 66, taked: 0.01197,train -> loss: 11.92151390289021, test -> loss: 11.73668,\n",
      "epoch 67, taked: 0.01124,train -> loss: 11.81840081085073, test -> loss: 11.83515,\n",
      "epoch 68, taked: 0.00997,train -> loss: 11.866667851363077, test -> loss: 11.46299,\n",
      "epoch 69, taked: 0.01097,train -> loss: 11.379213954838416, test -> loss: 11.5961,\n",
      "epoch 70, taked: 0.01193,train -> loss: 11.52206506781363, test -> loss: 11.35628,\n",
      "epoch 71, taked: 0.01197,train -> loss: 11.520026282508775, test -> loss: 11.17912,\n",
      "epoch 72, taked: 0.01097,train -> loss: 11.406532221501417, test -> loss: 11.01072,\n",
      "epoch 73, taked: 0.01082,train -> loss: 11.299158367347363, test -> loss: 10.97372,\n",
      "epoch 74, taked: 0.01197,train -> loss: 11.029488130371169, test -> loss: 10.95702,\n",
      "epoch 75, taked: 0.0112,train -> loss: 11.89085074698571, test -> loss: 10.85862,\n",
      "epoch 76, taked: 0.01001,train -> loss: 11.080256632058928, test -> loss: 10.61073,\n",
      "epoch 77, taked: 0.01196,train -> loss: 10.87934142764252, test -> loss: 10.58787,\n",
      "epoch 78, taked: 0.01101,train -> loss: 10.95648993123876, test -> loss: 10.51505,\n",
      "epoch 79, taked: 0.01001,train -> loss: 11.033659491208521, test -> loss: 10.45461,\n",
      "epoch 80, taked: 0.0116,train -> loss: 12.065675905435393, test -> loss: 10.26742,\n",
      "epoch 81, taked: 0.01097,train -> loss: 10.747362476764339, test -> loss: 10.39066,\n",
      "epoch 82, taked: 0.01097,train -> loss: 10.883079170000435, test -> loss: 10.26157,\n",
      "epoch 83, taked: 0.01128,train -> loss: 10.845912385695051, test -> loss: 10.17029,\n",
      "epoch 84, taked: 0.01097,train -> loss: 10.624122071974348, test -> loss: 10.13173,\n",
      "epoch 85, taked: 0.01097,train -> loss: 10.549858121588679, test -> loss: 10.11469,\n",
      "epoch 86, taked: 0.01097,train -> loss: 10.563528551913723, test -> loss: 9.98803,\n",
      "epoch 87, taked: 0.01001,train -> loss: 11.032897118294594, test -> loss: 10.04655,\n",
      "epoch 88, taked: 0.01197,train -> loss: 10.734359684557017, test -> loss: 10.06826,\n",
      "epoch 89, taked: 0.01158,train -> loss: 10.299374871177248, test -> loss: 9.8758,\n",
      "epoch 90, taked: 0.00997,train -> loss: 10.19851843083259, test -> loss: 10.07613,\n",
      "epoch 91, taked: 0.01147,train -> loss: 10.31982584179628, test -> loss: 9.7385,\n",
      "epoch 92, taked: 0.01093,train -> loss: 10.26492712285259, test -> loss: 9.99869,\n",
      "epoch 93, taked: 0.01097,train -> loss: 10.301419994618634, test -> loss: 9.89744,\n",
      "epoch 94, taked: 0.01107,train -> loss: 10.805146849981629, test -> loss: 9.57575,\n",
      "epoch 95, taked: 0.011,train -> loss: 10.727669725323668, test -> loss: 9.95592,\n",
      "epoch 96, taked: 0.01101,train -> loss: 10.274196568101939, test -> loss: 9.54192,\n",
      "epoch 97, taked: 0.01098,train -> loss: 9.908876930132951, test -> loss: 9.50099,\n",
      "epoch 98, taked: 0.01002,train -> loss: 10.0824132579388, test -> loss: 9.35642,\n",
      "epoch 99, taked: 0.00997,train -> loss: 10.113906784813002, test -> loss: 9.44378,\n",
      "epoch 100, taked: 0.01086,train -> loss: 17.87394716243933, test -> loss: 9.29063,\n",
      "epoch 101, taked: 0.01097,train -> loss: 10.025584891290947, test -> loss: 9.50275,\n",
      "epoch 102, taked: 0.01167,train -> loss: 9.978203438296177, test -> loss: 9.43537,\n",
      "epoch 103, taked: 0.01061,train -> loss: 10.128798239301927, test -> loss: 9.39527,\n",
      "epoch 104, taked: 0.01197,train -> loss: 10.002633878500154, test -> loss: 9.30338,\n",
      "epoch 105, taked: 0.01134,train -> loss: 10.83495865245857, test -> loss: 9.07893,\n",
      "epoch 106, taked: 0.00993,train -> loss: 9.673101914696174, test -> loss: 9.15092,\n",
      "epoch 107, taked: 0.00997,train -> loss: 9.783503749583026, test -> loss: 9.42087,\n",
      "epoch 108, taked: 0.01088,train -> loss: 10.024293710689733, test -> loss: 9.56419,\n",
      "epoch 109, taked: 0.00998,train -> loss: 10.80269156352128, test -> loss: 9.17275,\n",
      "epoch 110, taked: 0.01193,train -> loss: 10.055280553232326, test -> loss: 9.05246,\n",
      "epoch 111, taked: 0.0108,train -> loss: 9.761217932889958, test -> loss: 8.99018,\n",
      "epoch 112, taked: 0.01093,train -> loss: 10.358526135435199, test -> loss: 9.03333,\n",
      "epoch 113, taked: 0.01197,train -> loss: 16.84909762014257, test -> loss: 9.2528,\n",
      "epoch 114, taked: 0.01097,train -> loss: 10.365377237301061, test -> loss: 9.40857,\n",
      "epoch 115, taked: 0.01197,train -> loss: 10.290497033902914, test -> loss: 8.88834,\n",
      "epoch 116, taked: 0.01095,train -> loss: 9.718697269364158, test -> loss: 8.90945,\n",
      "epoch 117, taked: 0.01097,train -> loss: 10.304158881159111, test -> loss: 8.97825,\n",
      "epoch 118, taked: 0.01097,train -> loss: 9.547867208424181, test -> loss: 9.03842,\n",
      "epoch 119, taked: 0.0117,train -> loss: 10.002182327874817, test -> loss: 9.03526,\n",
      "epoch 120, taked: 0.01102,train -> loss: 9.653196330117707, test -> loss: 9.08086,\n",
      "epoch 121, taked: 0.012,train -> loss: 9.816359534121975, test -> loss: 9.01882,\n",
      "epoch 122, taked: 0.01201,train -> loss: 9.48845100992977, test -> loss: 8.87364,\n",
      "epoch 123, taked: 0.01193,train -> loss: 9.771644356227156, test -> loss: 8.82751,\n",
      "epoch 124, taked: 0.01101,train -> loss: 10.220759089630429, test -> loss: 8.89139,\n",
      "epoch 125, taked: 0.01001,train -> loss: 9.78205974503319, test -> loss: 8.87181,\n",
      "epoch 126, taked: 0.01101,train -> loss: 9.61829667516274, test -> loss: 8.91832,\n",
      "epoch 127, taked: 0.01203,train -> loss: 10.750469491033272, test -> loss: 8.76022,\n",
      "epoch 128, taked: 0.01094,train -> loss: 9.5725394870105, test -> loss: 8.80509,\n",
      "epoch 129, taked: 0.01197,train -> loss: 9.59962255647867, test -> loss: 8.85826,\n",
      "epoch 130, taked: 0.01197,train -> loss: 9.586292118129164, test -> loss: 8.76599,\n",
      "epoch 131, taked: 0.01098,train -> loss: 9.612353409871016, test -> loss: 8.67397,\n",
      "epoch 132, taked: 0.012,train -> loss: 9.44115460774686, test -> loss: 8.79281,\n",
      "epoch 133, taked: 0.012,train -> loss: 9.569118954757652, test -> loss: 8.59508,\n",
      "epoch 134, taked: 0.011,train -> loss: 10.170884406212533, test -> loss: 8.86218,\n",
      "epoch 135, taked: 0.01108,train -> loss: 10.380209875578927, test -> loss: 8.79438,\n",
      "epoch 136, taked: 0.01197,train -> loss: 9.591011812191198, test -> loss: 8.64656,\n",
      "epoch 137, taked: 0.012,train -> loss: 16.11159098030317, test -> loss: 8.59602,\n",
      "epoch 138, taked: 0.01076,train -> loss: 9.75410275412078, test -> loss: 8.70917,\n",
      "epoch 139, taked: 0.01096,train -> loss: 9.45273733139038, test -> loss: 8.794,\n",
      "epoch 140, taked: 0.01124,train -> loss: 9.620719720821569, test -> loss: 8.67556,\n",
      "epoch 141, taked: 0.01097,train -> loss: 9.692695712098981, test -> loss: 9.00125,\n",
      "epoch 142, taked: 0.01097,train -> loss: 9.531490694178213, test -> loss: 8.6193,\n",
      "epoch 143, taked: 0.01197,train -> loss: 9.589076438752732, test -> loss: 8.58996,\n",
      "epoch 144, taked: 0.00997,train -> loss: 9.542980250745718, test -> loss: 8.62186,\n",
      "epoch 145, taked: 0.01296,train -> loss: 10.428936080177232, test -> loss: 8.58999,\n",
      "epoch 146, taked: 0.01097,train -> loss: 10.471469425919032, test -> loss: 8.59152,\n",
      "epoch 147, taked: 0.012,train -> loss: 9.41624910996692, test -> loss: 8.60006,\n",
      "epoch 148, taked: 0.01197,train -> loss: 9.341892839068233, test -> loss: 8.48897,\n",
      "epoch 149, taked: 0.01193,train -> loss: 9.333293312847024, test -> loss: 8.53452,\n",
      "epoch 150, taked: 0.01213,train -> loss: 9.460540743157415, test -> loss: 8.57381,\n",
      "epoch 151, taked: 0.01197,train -> loss: 9.702202693070516, test -> loss: 8.60509,\n",
      "epoch 152, taked: 0.01197,train -> loss: 9.6686800777322, test -> loss: 8.55681,\n",
      "epoch 153, taked: 0.01097,train -> loss: 9.601481220509747, test -> loss: 8.64354,\n",
      "epoch 154, taked: 0.01097,train -> loss: 9.474574079607972, test -> loss: 8.58251,\n",
      "epoch 155, taked: 0.01197,train -> loss: 9.481971288376515, test -> loss: 8.6542,\n",
      "epoch 156, taked: 0.01197,train -> loss: 9.450355647814156, test -> loss: 8.52893,\n",
      "epoch 157, taked: 0.01098,train -> loss: 9.844571292990505, test -> loss: 8.61947,\n",
      "epoch 158, taked: 0.01101,train -> loss: 9.342189232901772, test -> loss: 8.60961,\n",
      "epoch 159, taked: 0.01193,train -> loss: 9.385252794269288, test -> loss: 8.59815,\n",
      "epoch 160, taked: 0.01097,train -> loss: 9.966825201959892, test -> loss: 8.55547,\n",
      "epoch 161, taked: 0.01197,train -> loss: 9.363689007145343, test -> loss: 8.57637,\n",
      "epoch 162, taked: 0.01097,train -> loss: 9.934963566241878, test -> loss: 8.64066,\n",
      "epoch 163, taked: 0.01201,train -> loss: 9.173954349933284, test -> loss: 8.45841,\n",
      "epoch 164, taked: 0.01097,train -> loss: 9.372857898861023, test -> loss: 8.57153,\n",
      "epoch 165, taked: 0.012,train -> loss: 9.348186625112401, test -> loss: 8.65428,\n",
      "epoch 166, taked: 0.012,train -> loss: 9.770556497101737, test -> loss: 8.41113,\n",
      "epoch 167, taked: 0.01297,train -> loss: 9.7803906355754, test -> loss: 8.58215,\n",
      "epoch 168, taked: 0.01197,train -> loss: 9.802150367510201, test -> loss: 8.5493,\n",
      "epoch 169, taked: 0.01197,train -> loss: 9.405713624118063, test -> loss: 8.61599,\n",
      "epoch 170, taked: 0.012,train -> loss: 10.105496151612536, test -> loss: 8.45487,\n",
      "epoch 171, taked: 0.01101,train -> loss: 9.63444992103199, test -> loss: 8.64655,\n",
      "epoch 172, taked: 0.01201,train -> loss: 9.533211023648187, test -> loss: 8.43821,\n",
      "epoch 173, taked: 0.012,train -> loss: 9.6044229941793, test -> loss: 8.41795,\n",
      "epoch 174, taked: 0.012,train -> loss: 9.783585425650719, test -> loss: 8.38116,\n",
      "epoch 175, taked: 0.01193,train -> loss: 9.599303443833152, test -> loss: 8.68622,\n",
      "epoch 176, taked: 0.011,train -> loss: 10.005559713533609, test -> loss: 8.40254,\n",
      "epoch 177, taked: 0.01197,train -> loss: 9.9294008878198, test -> loss: 8.51123,\n",
      "epoch 178, taked: 0.01197,train -> loss: 9.994357212935343, test -> loss: 8.46061,\n",
      "epoch 179, taked: 0.01101,train -> loss: 9.770510040887512, test -> loss: 8.39587,\n",
      "epoch 180, taked: 0.01101,train -> loss: 9.501540221790275, test -> loss: 8.38942,\n",
      "epoch 181, taked: 0.01197,train -> loss: 9.508140129618125, test -> loss: 8.49532,\n",
      "epoch 182, taked: 0.012,train -> loss: 10.028665958064618, test -> loss: 8.59067,\n",
      "epoch 183, taked: 0.01148,train -> loss: 9.927501489620397, test -> loss: 8.62778,\n",
      "epoch 184, taked: 0.01101,train -> loss: 9.87640116474416, test -> loss: 8.62643,\n",
      "epoch 185, taked: 0.01197,train -> loss: 9.461475334545174, test -> loss: 8.47928,\n",
      "epoch 186, taked: 0.01197,train -> loss: 9.367008218670836, test -> loss: 8.45133,\n",
      "epoch 187, taked: 0.01097,train -> loss: 10.533263178154973, test -> loss: 8.44852,\n",
      "epoch 188, taked: 0.01197,train -> loss: 15.459171975013053, test -> loss: 8.39947,\n",
      "epoch 189, taked: 0.012,train -> loss: 9.582857122515687, test -> loss: 8.47258,\n",
      "epoch 190, taked: 0.013,train -> loss: 9.231947421437443, test -> loss: 8.52083,\n",
      "epoch 191, taked: 0.01248,train -> loss: 9.305265653251421, test -> loss: 8.52431,\n",
      "epoch 192, taked: 0.012,train -> loss: 9.431209080290087, test -> loss: 8.72333,\n",
      "epoch 193, taked: 0.01097,train -> loss: 14.623531077167776, test -> loss: 8.63894,\n",
      "epoch 194, taked: 0.01097,train -> loss: 9.429957644774182, test -> loss: 8.38133,\n",
      "epoch 195, taked: 0.01197,train -> loss: 14.976686118852975, test -> loss: 8.3617,\n",
      "epoch 196, taked: 0.01197,train -> loss: 9.570571592538665, test -> loss: 8.58646,\n",
      "epoch 197, taked: 0.01101,train -> loss: 14.102635600779315, test -> loss: 8.6783,\n",
      "epoch 198, taked: 0.012,train -> loss: 9.078568844511958, test -> loss: 8.71968,\n",
      "epoch 199, taked: 0.01097,train -> loss: 9.64042390174795, test -> loss: 8.66173,\n",
      "epoch 200, taked: 0.01091,train -> loss: 9.792296910049892, test -> loss: 8.6275,\n",
      "epoch 201, taked: 0.011,train -> loss: 9.407119774582362, test -> loss: 8.59758,\n",
      "epoch 202, taked: 0.01192,train -> loss: 9.53820675493467, test -> loss: 8.42043,\n",
      "epoch 203, taked: 0.01096,train -> loss: 9.769500619114035, test -> loss: 8.46049,\n",
      "epoch 204, taked: 0.01097,train -> loss: 10.390310457437346, test -> loss: 8.51876,\n",
      "epoch 205, taked: 0.014,train -> loss: 9.870473181847299, test -> loss: 8.49668,\n",
      "epoch 206, taked: 0.01201,train -> loss: 9.71401571519304, test -> loss: 8.40679,\n",
      "epoch 207, taked: 0.01197,train -> loss: 14.874709610891815, test -> loss: 8.48988,\n",
      "epoch 208, taked: 0.013,train -> loss: 9.785342603626818, test -> loss: 8.57074,\n",
      "epoch 209, taked: 0.01396,train -> loss: 9.6194006334437, test -> loss: 8.3396,\n",
      "epoch 210, taked: 0.01293,train -> loss: 9.30634606002581, test -> loss: 8.34598,\n",
      "epoch 211, taked: 0.01197,train -> loss: 9.788146179501373, test -> loss: 8.2963,\n",
      "epoch 212, taked: 0.013,train -> loss: 9.63292373052918, test -> loss: 8.29134,\n",
      "epoch 213, taked: 0.01197,train -> loss: 9.5886122637456, test -> loss: 8.43534,\n",
      "epoch 214, taked: 0.012,train -> loss: 9.420733084773072, test -> loss: 8.38423,\n",
      "epoch 215, taked: 0.012,train -> loss: 9.905790064594534, test -> loss: 8.3182,\n",
      "epoch 216, taked: 0.012,train -> loss: 9.456988902965394, test -> loss: 8.34012,\n",
      "epoch 217, taked: 0.01197,train -> loss: 9.533534257718832, test -> loss: 8.35737,\n",
      "epoch 218, taked: 0.01097,train -> loss: 9.51169241536962, test -> loss: 8.34011,\n",
      "epoch 219, taked: 0.01197,train -> loss: 9.49081277375174, test -> loss: 8.54207,\n",
      "epoch 220, taked: 0.01101,train -> loss: 9.644729312103573, test -> loss: 8.48676,\n",
      "epoch 221, taked: 0.01097,train -> loss: 9.267568508144652, test -> loss: 8.30906,\n",
      "epoch 222, taked: 0.01093,train -> loss: 10.37466314523527, test -> loss: 8.24958,\n",
      "epoch 223, taked: 0.012,train -> loss: 10.18372372353431, test -> loss: 8.31039,\n",
      "epoch 224, taked: 0.01001,train -> loss: 9.592406565600102, test -> loss: 8.37123,\n",
      "epoch 225, taked: 0.01097,train -> loss: 9.646104189428952, test -> loss: 8.22755,\n",
      "epoch 226, taked: 0.01101,train -> loss: 9.589301741949402, test -> loss: 8.23051,\n",
      "epoch 227, taked: 0.01197,train -> loss: 14.781867754341352, test -> loss: 8.43296,\n",
      "epoch 228, taked: 0.011,train -> loss: 9.446015074701592, test -> loss: 8.41421,\n",
      "epoch 229, taked: 0.01197,train -> loss: 10.255316611563805, test -> loss: 8.43368,\n",
      "epoch 230, taked: 0.012,train -> loss: 9.456966330509374, test -> loss: 8.43966,\n",
      "epoch 231, taked: 0.01097,train -> loss: 14.91370969715685, test -> loss: 8.3619,\n",
      "epoch 232, taked: 0.01097,train -> loss: 14.427530911889408, test -> loss: 8.22595,\n",
      "epoch 233, taked: 0.011,train -> loss: 10.831013688946715, test -> loss: 8.34701,\n",
      "epoch 234, taked: 0.012,train -> loss: 10.173835310605494, test -> loss: 8.62811,\n",
      "epoch 235, taked: 0.01196,train -> loss: 10.096274857473846, test -> loss: 8.3417,\n",
      "epoch 236, taked: 0.01199,train -> loss: 9.363371027578221, test -> loss: 8.39202,\n",
      "epoch 237, taked: 0.0369,train -> loss: 9.715042571030041, test -> loss: 8.33105,\n",
      "epoch 238, taked: 0.012,train -> loss: 9.441058206086112, test -> loss: 8.41303,\n",
      "epoch 239, taked: 0.01297,train -> loss: 9.47034527995799, test -> loss: 8.61487,\n",
      "epoch 240, taked: 0.01197,train -> loss: 9.423922033593206, test -> loss: 8.36306,\n",
      "epoch 241, taked: 0.012,train -> loss: 9.76050072849387, test -> loss: 8.33555,\n",
      "epoch 242, taked: 0.012,train -> loss: 9.293255539223699, test -> loss: 8.62531,\n",
      "epoch 243, taked: 0.012,train -> loss: 9.542516626934013, test -> loss: 8.29119,\n",
      "epoch 244, taked: 0.01296,train -> loss: 16.021108570665415, test -> loss: 8.34643,\n",
      "epoch 245, taked: 0.01396,train -> loss: 9.509619986656869, test -> loss: 8.34748,\n",
      "epoch 246, taked: 0.014,train -> loss: 9.546851474459809, test -> loss: 8.25735,\n",
      "epoch 247, taked: 0.012,train -> loss: 9.628941616209426, test -> loss: 8.30239,\n",
      "epoch 248, taked: 0.01297,train -> loss: 9.486001156344273, test -> loss: 8.28703,\n",
      "epoch 249, taked: 0.011,train -> loss: 9.943040130162004, test -> loss: 8.66411,\n",
      "epoch 250, taked: 0.01193,train -> loss: 14.518543564447082, test -> loss: 8.54947,\n",
      "epoch 251, taked: 0.012,train -> loss: 9.494968997369899, test -> loss: 8.24674,\n",
      "epoch 252, taked: 0.01101,train -> loss: 10.093828635640664, test -> loss: 8.34399,\n",
      "epoch 253, taked: 0.01097,train -> loss: 9.624155667748783, test -> loss: 8.30671,\n",
      "epoch 254, taked: 0.01197,train -> loss: 10.210081534810586, test -> loss: 8.23879,\n",
      "epoch 255, taked: 0.01101,train -> loss: 9.473727468216774, test -> loss: 8.23055,\n",
      "epoch 256, taked: 0.01093,train -> loss: 9.54574710189706, test -> loss: 8.32901,\n",
      "epoch 257, taked: 0.011,train -> loss: 10.016876787242323, test -> loss: 8.34198,\n",
      "epoch 258, taked: 0.01197,train -> loss: 9.777506969942905, test -> loss: 8.40396,\n",
      "epoch 259, taked: 0.01197,train -> loss: 9.830368532992825, test -> loss: 8.31289,\n",
      "epoch 260, taked: 0.01097,train -> loss: 9.425237872813007, test -> loss: 8.17657,\n",
      "epoch 261, taked: 0.012,train -> loss: 9.37710148273128, test -> loss: 8.37323,\n",
      "epoch 262, taked: 0.011,train -> loss: 9.47715922157363, test -> loss: 8.32393,\n",
      "epoch 263, taked: 0.01101,train -> loss: 15.132155144568717, test -> loss: 8.27195,\n",
      "epoch 264, taked: 0.01001,train -> loss: 10.07729192299418, test -> loss: 8.3642,\n",
      "epoch 265, taked: 0.01001,train -> loss: 9.529407708951743, test -> loss: 8.39443,\n",
      "epoch 266, taked: 0.01097,train -> loss: 9.80116838511854, test -> loss: 8.37116,\n",
      "epoch 267, taked: 0.01097,train -> loss: 9.383023296252336, test -> loss: 8.47184,\n",
      "epoch 268, taked: 0.01098,train -> loss: 9.729304341986628, test -> loss: 8.30415,\n",
      "epoch 269, taked: 0.01101,train -> loss: 9.435775360258498, test -> loss: 8.33312,\n",
      "epoch 270, taked: 0.01197,train -> loss: 10.070922200042423, test -> loss: 8.60426,\n",
      "epoch 271, taked: 0.01197,train -> loss: 9.46086032555835, test -> loss: 8.26689,\n",
      "epoch 272, taked: 0.012,train -> loss: 9.519500642719835, test -> loss: 8.53336,\n",
      "epoch 273, taked: 0.015,train -> loss: 9.569441124944404, test -> loss: 8.26577,\n",
      "epoch 274, taked: 0.01097,train -> loss: 9.351993079232697, test -> loss: 8.41659,\n",
      "epoch 275, taked: 0.01197,train -> loss: 9.497949707626116, test -> loss: 8.25884,\n",
      "epoch 276, taked: 0.01197,train -> loss: 9.447159644400719, test -> loss: 8.40246,\n",
      "epoch 277, taked: 0.012,train -> loss: 9.617623281950998, test -> loss: 8.26544,\n",
      "epoch 278, taked: 0.01197,train -> loss: 10.143993339916268, test -> loss: 8.43296,\n",
      "epoch 279, taked: 0.01101,train -> loss: 15.197377724222617, test -> loss: 8.24744,\n",
      "epoch 280, taked: 0.01197,train -> loss: 10.525999654637705, test -> loss: 8.53268,\n",
      "epoch 281, taked: 0.013,train -> loss: 9.821257581805238, test -> loss: 8.42454,\n",
      "epoch 282, taked: 0.01101,train -> loss: 9.58667252323415, test -> loss: 8.39175,\n",
      "epoch 283, taked: 0.012,train -> loss: 9.583982236314528, test -> loss: 8.48319,\n",
      "epoch 284, taked: 0.01197,train -> loss: 9.491011156894192, test -> loss: 8.57834,\n",
      "epoch 285, taked: 0.01101,train -> loss: 9.188753407780487, test -> loss: 8.26204,\n",
      "epoch 286, taked: 0.01093,train -> loss: 10.289585680064588, test -> loss: 8.40142,\n",
      "epoch 287, taked: 0.013,train -> loss: 10.368342824501566, test -> loss: 8.21774,\n",
      "epoch 288, taked: 0.012,train -> loss: 9.363655718246308, test -> loss: 8.45021,\n",
      "epoch 289, taked: 0.01196,train -> loss: 9.347486864222159, test -> loss: 8.35588,\n",
      "epoch 290, taked: 0.01197,train -> loss: 9.354595071018332, test -> loss: 8.28789,\n",
      "epoch 291, taked: 0.01201,train -> loss: 10.026584058704943, test -> loss: 8.39856,\n",
      "epoch 292, taked: 0.012,train -> loss: 9.407533796707002, test -> loss: 8.34621,\n",
      "epoch 293, taked: 0.01197,train -> loss: 9.55344016717212, test -> loss: 8.39354,\n",
      "epoch 294, taked: 0.01148,train -> loss: 9.284154523717294, test -> loss: 8.32469,\n",
      "epoch 295, taked: 0.01197,train -> loss: 9.439914099060664, test -> loss: 8.36803,\n",
      "epoch 296, taked: 0.012,train -> loss: 9.857295876682395, test -> loss: 8.33413,\n",
      "epoch 297, taked: 0.01297,train -> loss: 9.301605704987404, test -> loss: 8.23562,\n",
      "epoch 298, taked: 0.01197,train -> loss: 9.320516007961613, test -> loss: 8.306,\n",
      "epoch 299, taked: 0.01097,train -> loss: 9.338093489703565, test -> loss: 8.30753,\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    start, train_loss, train_mse, train_n, test_loss, test_mse, test_n = time.time(), 0., 0., 0, 0., 0., 0\n",
    "    \n",
    "    # режим обучения\n",
    "    model.train()\n",
    "    for X, y in data_iter(batch_size, features_train, labels_train):\n",
    "        trainer.zero_grad() # не забывать обнуление градента!\n",
    "        y_hat = model.forward(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_losses.append(l.item())\n",
    "        train_loss += l.item()\n",
    "        train_n += len(X)\n",
    "    \n",
    "    # режим вычисления\n",
    "    model.eval()\n",
    "    for X, y in data_iter(batch_size, features_test, labels_test):\n",
    "        y_hat = model.forward(X)\n",
    "        l = loss(y_hat, y)\n",
    "        test_losses.append(l.item())\n",
    "        test_loss += l.item()\n",
    "        test_n += len(X)   \n",
    "    \n",
    "\n",
    "    print(f'epoch {epoch}, taked: {round(time.time() - start,5)},' \n",
    "          + f'train -> loss: {train_loss / train_n}, test -> loss: {round(test_loss / test_n, 5)},'\n",
    "          #+ f'w : {model[0].weight.data}, b :{model[0].bias.data}'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RM         0.409806\n",
       "RAD        0.381300\n",
       "B          0.250312\n",
       "ZN         0.195078\n",
       "TAX        0.129182\n",
       "CHAS       0.115291\n",
       "AGE        0.094818\n",
       "INDUS      0.093596\n",
       "CRIM      -0.231026\n",
       "PTRATIO   -0.373536\n",
       "NOX       -0.529726\n",
       "DIS       -0.529819\n",
       "LSTAT     -0.886847\n",
       "dtype: float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS','NOX','RM', 'AGE','DIS', 'RAD','TAX', 'PTRATIO','B', 'LSTAT', 'MEDV']  \n",
    "pd.Series(model[0].weight.data.numpy().flatten(), index=columns[:-1]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.450083], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAD        3.559998\n",
       "RM         1.691747\n",
       "AGE        1.332263\n",
       "CHAS       0.954143\n",
       "B          0.873937\n",
       "INDUS      0.730919\n",
       "ZN         0.722100\n",
       "CRIM      -0.811797\n",
       "TAX       -1.681608\n",
       "PTRATIO   -2.762897\n",
       "DIS       -3.614028\n",
       "NOX       -4.183770\n",
       "LSTAT     -5.461820\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "pd.Series(lr.coef_, index=columns[:-1]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.502970297029673"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
